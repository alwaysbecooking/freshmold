* Choosing Control Over Convenience
This document outlines the decision to use a combination of =psycopg3=, Pydantic models, and a dedicated SQL-first migration tool(Goose) for our application's database access layer. It explains the rationale for choosing this stack over alternatives like Object-Relational Mappers (ORMs) or code generators like =sqlc= (sqlc is great! but [[https://github.com/sqlc-dev/sqlc-gen-python/issues][python support sucks]]).

Finally, it suggests a strategy for mitigating the primary challenge of this approach: ensuring synchronization between the database schema and application's data models.

** The Chosen Stack
The database access layer is composed of three core components:
1. *psycopg3* : The high-performance, modern PostgreSQL driver for Python. It is used for all direct database communication.
2. *Pydantic* : Used to define our application's data models. It provides robust data validation, serialization, and clear, type-hinted data structures.
3. *Goose* : A dedicated, SQL-first migration tool. The SQL migration files are treated as the single source of truth for the database schema.

The core interaction pattern is to write raw SQL for maximum control and use =psycopg3='s built-in row factories to fetch data, which is then validated and parsed by Pydantic. It is important to note that Pydantic models must be manually written and kept in sync by inspecting the current database schema and migration files(See below for more on how to maintain the sync). This provides a clean, ergonomic, and type-safe bridge between the database and the application. Other times, whenever needed can be used ~dict_row~ and other ways to get data that ~psycopg3~ allows.

** Rationale and Justification
This decision prioritizes direct control, performance transparency, and a schema-first design philosophy. The bridge between the database and application code is explicitly managed, rather than relying on higher-level abstractions.
*** Why This Stack?
- *Full Control Over SQL* : An abstraction layer does not impose limitations. This allows for finely-tuned SQL to leverage the full power of PostgreSQL, including complex joins, window functions, and native data types that are critical for application features.
- *Performance Transparency* : There is no intermediate layer generating unpredictable or inefficient queries. The performance of our data access is directly tied to the SQL we write, making it straightforward to debug and optimize with standard tools like =EXPLAIN ANALYZE=.
- *Clear Separation of Concerns* : The database schema (managed by Goose migrations) and the application's data representation (Pydantic models) are treated as separate, explicit contracts. This clarity avoids the "Object-Relational Impedance Mismatch," where object-oriented patterns can conflict with the relational nature of SQL.
*** Alternatives Considered
Several other popular approaches were evaluated before finalizing this decision. While these tools are powerful in their own right, the chosen stack is a better fit for the specific principles of control and transparency.
**** Code Generation via =sqlc=
- *The Promise* : =sqlc= generates type-safe Python code directly from raw SQL queries, which is conceptually aligned with our desire to write SQL.
- *Our Assessment* : While =sqlc=, is great, the current state of the Python generator presented challenges for this specific use case. Certain PostgreSQL data types relied upon did not have the level of type-safe mapping required for production, which could lead to runtime errors. Given the need for deterministic type safety across the entire schema, the decision was made to wait for the tool to evolve further.
**** Object-Relational Mappers (ORMs)
- *The Promise* : ORMs like ~SQLAlchemy~ or ~SQLModel~ ([[https://www.reddit.com/r/Python/comments/1gtrfpf/sqlmodel_vs_sqlalchemy_for_production/][SQLAlchemy wrapper]]) excel at automating the translation between Python objects and database rows, significantly reducing boilerplate code.
- *Our Assessment* : An ORM was opted against for two main reasons:
  - *The Need to Bypass Abstractions* : In experience with complex applications, developers often need to drop down to raw SQL to solve difficult queries or optimize performance. In these scenarios, the ORM can become a "leaky abstraction" that adds cognitive overhead rather than reducing it.
  - *Preference for Schema-First Design* : SQL migration files are preferred as the ultimate source of truth for the database. Many ORM workflows encourage a "code-first" approach where Python classes define the schema. By keeping schema definition in SQL, a clear and authoritative record of the database structure is maintained over time.
** Mitigating the Core Challenge: Schema Synchronization
The primary risk of this chosen manual approach is the potential for the database schema and Pydantic models to drift out of sync. This is addressed with a two-pronged, automated strategy within the CI/CD pipeline.
*** Level 1: Automated Integration Testing
The test suite runs against a temporary PostgreSQL database that is built from migrations on every run. This process naturally catches synchronization errors:
- Pydantic ~ValidationError~ : If a query result is missing a field or has an incorrect data type, Pydantic will raise an error during model instantiation, failing the test.
- Database Errors : If a query references a column that a migration has removed or renamed, =psycopg3= will raise a database error, failing the test.

*** Level 2: Proactive Schema Introspection
#+begin_quote
This is something that we'll NOT do unless absolutely necessary, and honestly if this level of sync is required, I'd opt for some other solution but that's a story for another time.
#+end_quote
For an even stronger guarantee, a dedicated test script will be implemented that directly compares the live database schema against Pydantic models. This script will:
1. Connect to the test database after all migrations have been applied.
2. Query PostgreSQL's =information_schema.columns= to fetch the column names and types for a given table.
3. Introspect the corresponding Pydantic model's fields.
4. Assert that the two are in sync. Any discrepancy will fail the CI pipeline, forcing the developer to resolve the mismatch.

** Conclusion
The =psycopg3= + Pydantic stack, supported by Goose for migrations, provides an optimal balance of performance, control, and developer ergonomics. By carefully selecting tools that align with principles of transparency and schema-first design, full control over database interactions is retained. The risk of schema drift inherent in this approach is acknowledged and addressed proactively with a robust, automated testing and validation strategy, ensuring the long-term maintainability and reliability of the data access layer.

** Endnote
लिखा LLM ने है, लेकिन शब्द हमारे है।
