* Choosing Control Over Convenience
This document outlines the decision to use a combination of =psycopg3=, Pydantic models, and a dedicated SQL-first migration tool(Goose) for our application's database access layer. It explains the rationale for choosing this stack over alternatives like Object-Relational Mappers (ORMs) or code generators like =sqlc= (sqlc is great! but [[https://github.com/sqlc-dev/sqlc-gen-python/issues][python support sucks]]).

Finally, it suggests a strategy for mitigating the primary challenge of this approach: ensuring synchronization between our database schema and our application's data models.

** The Chosen Stack
Our database access layer is composed of three core components:
1. *psycopg3* : The high-performance, modern PostgreSQL driver for Python. It is used for all direct database communication.
2. *Pydantic* : Used to define our application's data models. It provides robust data validation, serialization, and clear, type-hinted data structures.
3. *Goose* : A dedicated, SQL-first migration tool. The SQL migration files are treated as the single source of truth for the database schema.

The core interaction pattern is to write raw SQL for maximum control and use =psycopg3='s built-in row factories to fetch data, which is then validated and parsed by Pydantic. This provides a clean, ergonomic, and type-safe bridge between the database and the application. Other times, whenever needed we can use ~dict_row~ and other ways to get data that ~psycopg3~ allows.

** Rationale and Justification
This decision prioritizes direct control, performance transparency, and a schema-first design philosophy. We are consciously choosing to manage the bridge between our database and application code explicitly, rather than relying on higher-level abstractions.
*** Why This Stack?
- *Full Control Over SQL* : We are never limited by an abstraction layer. This allows us to write finely-tuned SQL to leverage the full power of PostgreSQL, including complex joins, window functions, and native data types that are critical for our application's features.
- *Performance Transparency* : There is no intermediate layer generating unpredictable or inefficient queries. The performance of our data access is directly tied to the SQL we write, making it straightforward to debug and optimize with standard tools like =EXPLAIN ANALYZE=.
- *Clear Separation of Concerns* : The database schema (managed by Goose migrations) and the application's data representation (Pydantic models) are treated as separate, explicit contracts. This clarity avoids the "Object-Relational Impedance Mismatch," where object-oriented patterns can conflict with the relational nature of SQL.
*** Alternatives Considered
We evaluated several other popular approaches before finalizing our decision. While these tools are powerful in their own right, we found our chosen stack to be a better fit for our specific principles of control and transparency.
**** Code Generation via =sqlc=
   - *The Promise* :: =sqlc= generates type-safe Python code directly from raw SQL queries, which is conceptually aligned with our desire to write SQL.
   - *Our Assessment* :: While we are excited about the future of =sqlc=, we found that the current state of the Python generator presented challenges for our specific use case. Certain PostgreSQL data types we rely on did not have the level of type-safe mapping we require for production, which could lead to runtime errors.[fn:1] Given our need for deterministic type safety across our entire schema, we decided to wait for the tool to evolve further.

**** 2.2.2. Object-Relational Mappers (ORMs)
   - *The Promise* :: ORMs like SQLAlchemy or SQLModel excel at automating the translation between Python objects and database rows, significantly reducing boilerplate code.
   - *Our Assessment* :: We opted against an ORM for two main reasons:
     1.  *The Need to Bypass Abstractions* :: In our experience with complex applications, developers often need to drop down to raw SQL to solve difficult queries or optimize performance. In these scenarios, the ORM can become a "leaky abstraction" that adds cognitive overhead rather than reducing it.
     2.  *Preference for Schema-First Design* :: We prefer that our SQL migration files serve as the ultimate source of truth for our database. Many ORM workflows encourage a "code-first" approach where Python classes define the schema. By keeping schema definition in SQL, we maintain a clear and authoritative record of our database structure over time.

** 3. Mitigating the Core Challenge: Schema Synchronization
The primary risk of our chosen manual approach is the potential for the database schema and our Pydantic models to drift out of sync. We address this with a two-pronged, automated strategy within our CI/CD pipeline.

*** 3.1. Level 1: Automated Integration Testing
Our test suite runs against a temporary PostgreSQL database that is built from our migrations on every run. This process naturally catches synchronization errors:
- *Pydantic `ValidationError`* :: If a query result is missing a field or has an incorrect data type, Pydantic will raise an error during model instantiation, failing the test.
- *Database Errors* :: If a query references a column that a migration has removed or renamed, =psycopg3= will raise a database error, failing the test.

*** 3.2. Level 2: Proactive Schema Introspection
For an even stronger guarantee, we will implement a dedicated test script that directly compares the live database schema against our Pydantic models. This script will:
1. Connect to the test database after all migrations have been applied.
2. Query PostgreSQL's =information_schema.columns= to fetch the column names and types for a given table.
3. Introspect the corresponding Pydantic model's fields.
4. Assert that the two are in sync. Any discrepancy will fail the CI pipeline, forcing the developer to resolve the mismatch.

** 4. Conclusion
The =psycopg3= + Pydantic stack, supported by Goose for migrations, provides our team with an optimal balance of performance, control, and developer ergonomics. By carefully selecting tools that align with our principles of transparency and schema-first design, we retain full control over our database interactions. We acknowledge the risk of schema drift inherent in this approach and address it proactively with a robust, automated testing and validation strategy, ensuring the long-term maintainability and reliability of our data access layer.
